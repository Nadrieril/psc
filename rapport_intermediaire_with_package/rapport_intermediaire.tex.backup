\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}%%seul package à charger~: pour éviter les problèmes sordides d'encodage
\usepackage[rapport,psc]{andre}%%bon eh bien sûr...

\usepackage{makeidx}              %% permet de générer un index automatiquement
\usepackage[style=numeric,backend=bibtex]{biblatex}				%% Utilisé pour la biblio



\title{Projet Scientifique Collectif X2013 \\INF02~: Synthétiseur automatique de documents} 
\renewcommand{\petittitre}{Projet INF02}
\newcommand{\pyt}[1]{\texttt{#1}}
\newcommand{\ang}[1]{\textit{#1}}%noms anglais
\author{\membres} %%\membres uniquement avec l'option psc
\date{21 janvier 2015}

%mettre les accents comme \c{c}a, sinon risque de plantage
\index{R\'eseau S\'emantique}
\index{Analyse Symbolique}
\index{Analyse S\'emantique}
\index{R\'eseau de Concepts}

\makeindex
\addbibresource{biblio.bib}
\begin{document}

\titrelong{}


\tableofcontents			
\newpage

\section{Rappel du projet}

\subsection{Notre groupe}
\begin{itemize}
 \item Fernandes-Pinto-Fachada Sarah, \textbf{8\textsuperscript{e}} compagnie, section \textbf{\'equitation};
 \item Schrottenloher Andr\'e, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Angibault Antonin, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Hufschmitt Th\'eophane, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Cao Zhixing, \textbf{9\textsuperscript{e}} compagnie, section \textbf{escalade};
 \item Boisseau Guillaume, \textbf{6\textsuperscript{e}} compagnie, section \textbf{natation};
\end{itemize}

\subsection{Notre sujet}

\subsubsection{But du projet}

Le but de notre sujet est de produire un programme capable d'analyser s\'emantiquement\index{Analyse S\'emantique} un texte ayant pour sujet le sport (et plus particulièrement encore le football) de façon à en produire un résumé. Il s'agit là d'une véritable innovation par rapport à l'état actuel du résumé automatique, dans la mesure où les programmes existants se basent prioritairement sur une analyse statistique plus ou moins poussée de fréquence d'apparition des mots pour juger de leur importance~; notre programme se baserait plutôt sur le sens pour juger de la pertinence d'un concept et savoir s'il doit le faire figurer ou non dans le résumé final.

Dans l'état actuel des choses, nous n'espérons pas produire un programme utilisable dans l'industrie ou plus efficace que ceux déjà existants~; nous pensons en revanche que cette autre manière d'aborder le résumé automatique est plus viable, dans la mesure où elle analyse plus directement que par la méthode statistique.

\subsubsection{Modules du projet et répartition du travail}
Pour atteindre cet objectif, plusieurs modules assez indépendants ont pu être identifiés et traités séparément~:

\begin{description}
	\item[R\'ecup\'eration de donn\'ees] à utiliser pour la création d'un réseau de concepts modélisant la connaissance \textit{a priori} du programme. Responsables : Théophane Hufschmitt, Guillaume Boisseau.
	\item[Création du réseau de concepts] et remplissage de celui-ci. Il s'agissait ici d'une part de définir sa structure et d'autre part d'y introduire les données nécessaires. Responsables : André Schrottenloher, Guillaume Boisseau.
	\item[Analyse syntaxique] à l'aide d'outil existants. Il s'agissait de trouver une grammaire capable d'analyser une phrase et d'en faire ressortir les relations de type verbe-objet ou nom-qualificatif. Responsables : Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada
	\item[Réflexion] sur l'utilisation du réseau de concepts et de l'analyse grammaticale pour produire un réseau capable de rendre compte de la compréhension du texte. Responsables : tous.
	\item[Implémentation] d'un algorithme simple de résumé stastique (basé sur l'algorithme TF-IDF), devant servir de base de comparaison pour tester la qualité des résumés produits par notre algorithme.  Responsable : Zhixing Cao. 
\end{description}

\subsubsection{Outils extérieurs utilisés}

Le volume de travail représenté par la création et l'utilisation du réseau de concepts\index{R\'eseau de Concepts} est tel que nous nous appuyons sur un certain nombre d'outils pour traiter le texte en amont de notre résumé. La construction du réseau s'appuie, elle aussi, sur des bases de connaissances déjà existantes (et libres) dont voici la liste :

\begin{itemize}
	\item[WordNet~: ]Dictionnaire permettant d'obtenir la classe grammaticale des mots présents dans le réseau.
	\item[Conceptnet~: ]Grande base de connaissances permettant d'établir la proximité conceptuelle entre différents termes du réseau.
	\item[Freebase~: ]Cette base de connaissance permet d'identifier les noms propres, tels que les joueurs de foot ou les différents stades.
	\item[Natural Language ToolKit (\pyt{nltk})~: ]Divers outils grammaticaux pour l'étude du langage naturel.
\end{itemize}

\section{\'Etat d'avancement~: r\'ecup\'eration de donn\'ees}

Nous r\'ecup\'erons des donn\'ees sur diff\'erents sites internet et flux rss, en moyenne 400 articles de sports par jour.

Ces donn\'ees vont servir \`a diff\'erents niveaux :
\begin{itemize}
 \item Dans la cr\'eation du r\'eseau de concepts (partie suivante) ;
 \item Dans la production de r\'esum\'es automatiques.
\end{itemize}


\section{Le r\'eseau de concepts}


\subsection{Structure}


\begin{definition}[Réseau de concepts]
Le réseau de concepts\index{R\'eseau de Concepts} est la ``mémoire à long terme'' de notre programme.

Ce réseau est au carrefour entre les réseaux sémantiques et les réseaux de neurones. Les n\oe{}uds du réseau ne représentent pas des concepts à proprement parler~; en revanche, les concepts sont vus comme regroupant des n\oe{}uds fortement interconnectés.
\end{definition}


Le R\'eseau est constitué de n\oe{}uds. Chaque n\oe{}ud comporte~:
\begin{itemize}
  \item Une étiquette (mot)~;
 \item Une importance conceptuelle (IC) ou profondeur conceptuelle~;
 \item Une activation (A) initialement nulle~;
 \item Un certain nombre de liens à d'autres n\oe{}uds.
\end{itemize}

Chaque lien comporte~:
\begin{itemize}
 \item Une proximité conceptuelle (ou poids W)~;
 \item Une étiquette qui le caract\'erise.
\end{itemize}

\subsection{Outils pour la construction}

Le but est donc de créer un réseau de concepts de base, que nous puissions utiliser comme base de connaissances dans le domaine considéré (ici le sport, mais nous pourrions avoir sélectionné un autre domaine).

Nous utilisons deux types de sources~:
\begin{itemize}
 \item Des articles de sport (données brutes) qui contiennent entre autres ``ce qu'il faut savoir''~;
 \item Des données sémantiques libres.
\end{itemize}


\subsubsection{Traitement de donn\'ees textuelles}

Les concepts que nous ins\'ererons dans le r\'eseau seront des mots sous forme normale.

\begin{definition}[Forme normale]
La forme normale d'un mot\index{Forme normale} est tout simplement celle qu'il a lorsqu'on le recherche dans le dictionnaire, par exemple :
\begin{itemize}
 \item Si c'est un verbe, on le met \`a l'infinitif ;
 \item Au singulier si c'est un nom.
\end{itemize}
Mettre un mot sous forme normale permet de s'int\'eresser uniquement au sens du mot hors de tout contexte.
\end{definition}


\begin{definition}[Part-of-speech tagger]
Le Part-of-speech tagger\index{Part-of-speech tagger}, abr\'eg\'e en POS, est un programme, le plus souvent entra\^in\'e sur de grandes bases de donn\'ees, qui permet d'associer \`a chaque mot dans une phrase une valeur grammaticale.
\end{definition}

\`A partir d'articles consacr\'es au sport (texte brut), les manipulations suivantes permettent de r\'ecup\'erer les noms propres et les concepts sous forme normale :
\begin{itemize}
 \item Découpage du texte en phrases et en mots (\textit{tokens})~;
 \item Utilisation d'un POS, celui de \pyt{nltk}\index{Natural Language ToolKit} par d\'efaut ~;
 \item Récupération des noms propres et mise à part de ceux-ci (qui doivent faire plus tard l'objet d'un traitement particulier)~;
 \item Suppression des conjonctions de coordination, pronoms et autres mots qui ne sont pas de v\'eritables concepts~;
 \item Il reste alors des adjectifs, adverbes, noms et verbes. Nous utilisons \pyt{morphy} de WordNet\index{WordNet} \`a l'int\'erieur d'un code d\'edi\'e \`a la construction de Conceptnet5 (voir partie suivante), pour transformer chaque terme en sa forme normale ;
 \item Les termes restants sont alors tous les concepts auxquels le texte fait appel ;
 \item Nous ne sommes pas \`a l'abri d'erreurs li\'ees \`a l'une des \'etapes. De manière générale, un concept qui apparaît au moins deux fois dans l'ensemble du corpus considéré peut être considéré comme valide.
\end{itemize}


\subsubsection{WordNet}

WordNet est une base de donn\'ees lexicale pour l'anglais, produite par l'universit\'e de Princeton.

Nous l'utilisons à travers son interface \pyt{nltk}, pour récupérer des informations sur les mots et les mettre sous forme normale.


\subsubsection{Conceptnet5}

Conceptnet (<http://conceptnet5.media.mit.edu/>) est un projet libre de réseau sémantique représentant des connaissances usuelles, aussi bien de la vie de tous les jours que culturelles et scientifiques. Il fait partie du \ang{Commonsense Computing Initiative} qui relie différents laboratoires, dont le MIT Media Lab, et entreprises.

Il est important de noter que conceptnet5 est généré à partir de données brutes. Conceptnet5 est relié à DBPedia, une grande partie de ses connaissances provient de Wiktionary, une partie de WordNet.

Nous avons utilis\'e une partie de code source provenant de la page <https://github.com/commonsense/conceptnet5>.


\subsubsection{Structure de Conceptnet5}

Conceptnet5\index{Conceptnet5} est un graphe sémantique contenant plusieurs millions de nœuds et d'ar\^etes.

les n\oe{}uds sont des mots et de courtes phrases, pas seulement en anglais. Les arêtes qui relient ces n\oe{}uds expriment une connaissance, elles contiennent chacune une relation particulière.

Une arête possède aussi une source (d'où provient l'information) et un poids en fonction de cette source, selon l'importance de l'arête.

Une relation concept-arête-concept exprime une assertion. Une même assertion peut être exprimée de différentes manières.

Une assertion peut elle-même être utilisée comme un n\oe{}ud ou comme une arête (on peut avoir des assertions d'assertions).

Les relations valables dans tout langage, pour ConceptNet5, sont par exemple~: 
\begin{itemize}
 \item RelatedTo ;
 \item IsA ;
 \item PartOf ;
 \item MemberOf ;
 \item HasA ;
 \item TranslationOf ;
 \item DefinedAs...
\end{itemize}

Il en existe beaucoup d'autres.

Comme Conceptnet est construit en partie \`a l'aide de WordNet, il existe une correspondance entre relations de l'un et de l'autre :

La correspondance entre relations de WordNet et de Conceptnet est par exemple~:
\begin{itemize}
 \item `attribute': `Attribute' ;
 \item `causes': `Causes' ;
 \item `classifiedByRegion': `HasContext' ;
 \item `hyponymOf': `IsA' ;
 \item `sameVerbGroupAs': `SimilarTo' ;
 \item `seeAlso': `RelatedTo'.
\end{itemize}

Nous avons décidé après coup de conserver ces relations dans le réseau de concepts, et d'en utiliser un sous-ensemble.

\subsubsection{Détail de l'API}

Nous pouvons faire un certain nombre de requêtes à Conceptnet5. Il existe trois types différents de requêtes~:
\begin{itemize}
 \item Lookup ;
 \item Association ;
 \item Search ;
\end{itemize}
Association permet de calculer la proximité entre deux concepts, ou de récupérer une liste de concepts proches d'un concept donné.

Search permet de récupérer une liste d'arêtes (\ang{edges}) entre concepts, selon les paramètres spécifiés (le plus souvent, on impose le concept de départ \ang{start} ou d'arrivée \ang{end}). 

Lookup permet d'analyser un concept (on aura par exemple accès à des listes d'arêtes dans lequel il intervient).


\subsubsection{Utilisation de Conceptnet5}

Nous utilisons en majorité Association et Search.

Étant donné un concept dont nous savons qu'il doit être étendu, nous utilisons conceptnet5 pour créer de nouveaux concepts au sein du réseau, et pour ajouter de nouveaux liens. Une première requête de type Search permet d'avoir accès à un certain nombre de liens vers d'autres concepts, qui sont ajoutés (on ne récupère que les plus pertinents). Une requête de type Association permet de créer de nouveaux liens vers d'autres concepts similaires.

Nous ajoutons alors une arête SimilarTo.
    
\subsubsection{Freebase}

Freebase\index{Freebase} est une immense base de données sémantiques qui contient beaucoup d'informations, sur des noms propres notamment. Le projet, repris par google mais sous une license qui laisse les données libres d'accès, de téléchargement et d'utilisation, a lui aussi une API, un peu plus complexe que celle de Conceptnet5. Freebase permet notamment de récup\'erer des informations sur une personne à partir de son nom, et à peu de frais, de vérifier que cette personne existe bel et bien.


\subsection{Construction du r\'eseau et r\'esultats}

\`A l'aide des outils susmentionn\'es, nous avons \'et\'es \`a m\^eme de construire un premier r\'eseau de concepts.
Nous pr\'esentons dans cette partie les caract\'eristiques du r\'eseau construit, ainsi que les choix de programmation effectu\'es.

\subsubsection{R\'ecup\'eration de concepts et de noms}

Nous analysons un certain nombre d'articles, ici 3043 issus 7 flux rss diff\'erents sur 16 jours.

Nous utilisons en partie les \ang{tokenizers} de \pyt{nltk}, et nous compl\'etons si besoin.

Pour r\'ecup\'erer les noms propres, nous rep\'erons \`a l'int\'erieur d'un article les mots marqu\'es comme noms propres par le POS de \pyt{nltk}. Puis nous concat\'enons deux noms propres successifs. Enfin, nous retirons de la liste des noms propres ceux qui sont contenus \`a l'int\'erieur d'autres. Cette \'etape, largement sous-estim\'ee au d\'ebut, est cruciale puisqu'elle \'evite de consid\'erer "< nom + pr\'enom "> et "<pr\'enom"> comme deux personnes diff\'erentes.

Nous comptons aussi la fr\'equence d'apparition de chaque concept. On obtient 9744 noms et environ 12700 concepts, ce qui est tr\`es raisonnable.

\subsubsection{Traitement des noms avec Freebase}

Nous n'utilisons pour l'instant l'API Freebase que de fa\c{c}on tr\`es superficielle. Chaque nom propre donne lieu \`a une requ\^ete. Si celle-ci aboutit, on analyse le r\'esultat et si celui-ci comporte une donn\'ee (exemple : \`a l'envoi de \verb|"scott_jamieson"|, on r\'ecup\`ere \verb|"soccer_midfielder"|), on ajoute un nouveau nœud correspondant au r\'eseau, et une ar\^ete "<IsA"> dont le poids d\'epend du r\'esultat de la requ\^ete.

Dans notre exemple, 676 noms seront rejet\'es par Freebase : c'\'etaient donc des r\'esultats erron\'es de la premi\`ere \'etape.

Environ 7500 nœuds sont cr\'e\'es dans le r\'eseau.

\subsubsection{Traitement des concepts avec Conceptnet5}

Pour chaque concept dans la liste, on envoie deux requ\^etes : l'une vise \`a trouver des ar\^etes qui partent de lui (Search), l'autre \`a trouver des concepts proches (Association). 

En l'absence de r\'esultat, le concept ne sera pas ajout\'e au r\'eseau.

Dans le cas contraire, de nouvelles ar\^etes sont cr\'e\'ees. Leurs poids d\'ependent des r\'esultats des requ\^etes. Pour la requ\^ete Association, nous mettons une relation "<SimilarTo">.

L'importance conceptuelle d'un nouveau nœud d\'epend essentiellement de son nombre d'apparitions dans les articles. D'autres possibilit\'es sont envisageables (nombre de synonymes avec WordNet, par exemple).

Dans notre exemple, sur les 12700 concepts, 4800 ne donnent lieu \`a aucun r\'esultat. Cela peut para\^itre beaucoup ; en r\'ealit\'e, quand on regarde de plus pr\`es la "<liste noire"> (que nous stockons), on a par exemple : "reimagining","money-grabbers","whoscored","impeccable","amateurish","dawkins","rat-a-tat","single-handedly","petered","real-politik","re-tweet","throwing"

Ces mots sont soit compos\'es (et donc difficiles \`a analyser), soit r\'esultent d'erreurs du POS-tagger (on voit passer quelques noms propres).

Cette liste pourra bien s\^ur donner lieu \`a une analyse plus fine (nouvelles requ\^etes Freebase).

\`A ce stade, notre r\'eseau poss\`ede 24000 nœuds environ, ce qui reste modique.

\subsubsection{Extension des nœuds du r\'eseau}

Nous envisageons maintenant, par de nouvelles requ\^etes, d'\'etendre les nœuds que nous venons de cr\'eer. Le but est que le r\'eseau soit au mieux interconnect\'e, or de nombreux nœuds r\'esultant des requ\^etes pr\'ec\'edentes ne pointent vers aucun nœud.

D'autres op\'erations sont possibles.

Nous regardons 5000 de ces nœuds faiblement connect\'es. Apr\`es requ\^etes, 2300 sont reli\'es au r\'eseau. (Cette \'etape a \'et\'e particuli\`erement lente).

\subsubsection{Suppression des nœuds inutiles}

Malgr\'e les nombreux param\`etres servant \`a filtrer, les requ\^etes n'ont pas donn\'e des r\'esultats toujours pertinents. Nous choisissons de retirer du r\'eseau les nœuds faiblement reli\'es (un seul voisin). Ceux-ci seront de toute fa\c{c}on peu utiles lors de la propagation de l'activation.

\`A  ce stade, le r\'eseau comporte 24000 nœuds et 35000 ar\^etes. Apr\`es nettoyage, il reste 17500 nœuds et 29000 ar\^etes.

Nous disposons maintenant d'un r\'eseau utilisable (en th\'eorie).

\subsubsection{Premiers tests avec ce r\'eseau}

Pour chaque test, nous activons un nœud et laissons l'activation se propager \`a ses successeurs au cours du temps (sans d\'esactivation). \`A chaque \'etape (nouvel instant) se produit un calcul d'activation.

\begin{itemize}
 \item \verb|"manchester", 60|
 \begin{verbatim}
  [...]
  Time=10
  comic_book_character : 1 ; furniture : 4
  house : 5 ; manchester : 60
  object : 9 ; place : 11
  thing : 6 ; desk : 4
  location : 21 ; loft : 3
 \end{verbatim}
\item \verb|"francis_lastic",60|
\begin{verbatim}
  [...]
  Time=5
  coach : 23 ; francis_lastic : 60
\end{verbatim}
\item \verb|"ball_sport",60|
\begin{verbatim}
  [...]
  Time=10
  ball_sport : 60 ; team_sport : 40
  basketball : 40 ; interest : 7
  football : 40
\end{verbatim}
\end{itemize}

\subsection{Choix de programmation et prospectives}

Nous avons d'abord cru que l'utilisation massive de requ\^etes donnerait des contraintes de temps importantes \`a nos programmes. C'est en r\'ealit\'e une erreur. La parall\'elisation des requ\^etes, que nous avons commenc\'e \`a effectuer, permet un gain de temps exceptionnel et va nous conduire dans les prochaines semaines \`a am\'eliorer consid\'erablement le temps d'ex\'ecution.

Le r\'eseau est pour le moment enregistr\'e sous forme de JSON Stream, un type de donn\'ees repris \`a Conceptnet5. Le nombre de nœuds \'etant rest\'e raisonnable, nous n'avons pas eu \`a abandonner pr\'ematur\'ement le package \pyt{networkx} qui nous permet de le repr\'esenter en m\'emoire.

Cependant, nous envisageons de passer \`a une base de donn\'ees.

Enfin, nous nous orientons maintenant vers des fonctions permettant de construire incr\'ementalement le r\'eseau, donc d'ajouter les nouveaux concepts et noms propres au fur et \`a mesure qu'ils sont lus dans les articles (notre but initial \'etait naturellement de construire un r\'eseau "<de base">).

\section{Traitement du r\'eseau}

Le réseau de concepts représente, comme nous l'avons déjà dit, la connaissance \textit{a priori} de notre programme sur le domaine étudié. Pour étudier un texte particulier, il s'agit maintenant de faire agir ce texte sur nos connaissances et d'interpréter l'effet que cette lecture a.

Deux méthodes ont été envisagées : la première étudiait, essentiellement, la différence entre l'état du réseau de concepts avant et après la lecture et devait en déduire les faits importants. La seconde, qui a finalement été retenue, s'appuie sur une activation des concepts dans le réseau, puis une instanciation des concepts fortement activés dans un espace de travail appelé \textit{workspace}\index{Workspace}. Ces concepts instanciés sont par la suite capable d'effectuer des tâches plus complexes relatives à la compréhension du texte.

Quelle que soit la méthode employée, une analyse syntaxique\index{Analyse syntaxique} préalable du texte est nécessaire pour rendre compte de sa compréhension et utiliser au mieux notre réseau de connaissance.

\subsection{Workspace}\index{Workspace}



\subsection{Analyse syntaxique}\index{Analyse syntaxique}

Cette partie ne constituant pas le c\oe{}r de notre psc, nous avons décidé d'utiliser un outil déjà codé, faisant partie de la plus grande librairie libre \pyt{nltk}\index{Natural Language ToolKit}.

\subsubsection{Description algorithmique d'une grammaire}\index{Grammaire}
Une grammairep peut être décrite informatiquement de manière très simple. En effet, on peut la voir comme~:
\begin{itemize}
	\item Un ensemble de classes terminales (verbe, nom par exemple) associées à une liste de mots ou éventuellement d'expressions
	\item Un ensemble de règles, souvent sous la forme d'une reconnaissance de motifs, permettant de découper une phrase en plusieurs éléments (et finalement en unités syntaxiques élémentaires).
\end{itemize}

Il y a par conséquent deux manières pour une grammaire d'être incomplète~: soit elle ne connaît pas assez de vocabulaire (manque d'éléments dans les cas terminaux), soit elle ne connaît pas assez de règles. La première faille est assez facile à combler par des requêtes vers WordNet\index{WordNet} (et par ailleurs un outil automatique est inclus dans \pyt{nltk}\index{Natural Language ToolKit}~; il est en revanche assez indispensable d'avoir une grammaire complète en termes de règles car il est beaucoup plus difficile d'inventer des manières de découper une phrase.

\subsubsection{\'Etat du texte en fin d'analyse}
Les grammaires nltk sont classées selon la forme de leur sortie. Elles attendent la liste des mots dans l'ordre dans lequel ils apparaissent dans le texte à étudier, accompagnés d'une liste de classes possibles pour ces mots. En sortie, on peut trouver~:

\begin{itemize}
	\item Un arbre représentant une structure grammaticale possible pour la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase, et pour chaque arbre une mesure de la vraisemblance de cette structure.
\end{itemize} 

Il y a une grande différence entre les n\oe{}uds non terminaux des arbres en sortie et leurs feuilles. En effet, ces dernières seront les unités syntaxiques présentes à l'origine dans la phrase, tandis que les n\oe{}uds internes représentent la classe grammaticale des bouts de phrases représentés par les sous-arbres dont ils sont la racine.

%Un exemple serait sûrement le bienvenu, je vais voir si je me chauffe pour faire un ParseTree a l'air plausible. (Antonin)
%Cela étant n'hésitez pas à vous chauffer vous aussi.

Les n\oe{}uds internes sont en nombre fini et leurs étiquettes dépend de si on considère des grammaires sans contexte (capables de séparer des composants autour d'un verbe) ou des grammaires à dépendance (dont le but est d'identifier le sujet et les compléments du verbe central).


\section{Suites envisag\'ees}

Dans l'ensemble le projet avance avec peu de retard sur le plan prévisionnel écrit en novembre dernier. Le réseau de concepts est prêt à être utilisé et notre attention pourra se reporter sur le développement du \textit{workspace}\index{Workspace} et du déploiement de l'analyse syntaxique.

Cette dernière, que nous avions prise pour à peu près acquise, s'avère requérir plus de travail que prévu (il nous faudra l'entraîner et la préparer un peu, ce qui requiert une certaine compréhension des mécanismes qui la sous-tendent).

La planification pour le reste du projet est la suivante~:
\begin{description}
	\item[Février~: ]Mise en place d'une grammaire \pyt{nltk}. Responsables : Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada.
	\item[Février~: ]Fin de la spécification du contenu du \textit{workspace}.
	\item[Février-Avril~: ]Implémentation du \textit{workspace} (donc, essentiellement, de ce qui manque à l'algorithme de résumé pour fonctionner).
	\item[Avril~: ]Génération de résumé et évaluation de ceux-ci.
\end{description}

Les différences essentielles avec la planification initiale proviennent de la complexité inattendue de la grammaire. En revanche, le temps d'exécution prévu pour la production d'un résumé d'article étant long, nous produirons finalement peu de résumés à évaluer, cela nous permet donc de les évaluer à la main et de gagner du temps sur cette partie de la planification initiale.

Globalement nous pensons être capables de mener le projet à bien comme prévu. Les sections suivantes décrivent les points les plus prêts d'être mis en place.

\subsection{Apprentissage du réseau de concepts}
Le réseau de concept représente un \textit{a priori} sur le monde de notre programme. Il est donc normal que cette structure évolue au fur et à mesure des textes lus. 

Les fonctions de construction du réseau peuvent sans mal être appelées après la génération du résumé pour y apporter des modifications pérennes.

\subsection{\'Evaluation des résumés}
Le faible nombre de résumés que nous prévoyons de produire nous permettra d'utiliser la méthode la plus simple parmi celles qui étaient mentionnées dans la proposition détaillée~: la comparaison à un résumé déclaré bon car produit par un humain.

L'idée est la suivante~: l'un de nous résumera un texte, nous emploierons notre programme pour résumer le même texte, et enfin nous utiliserons un simple algorithme statistique (basé sur TF-IDF) pour résumer ce même texte. Nous comparerons ensuite ces résumés du point de vue de l'information qu'ils contiennent~: sont-ils des images assez fidèles du document original ?

Compte-tenu de la simplicité de l'algorithme statistique, nous espérons que le résumé produit par notre programme se situera quelque part entre le résumé statistique et le résumé humain.

\section{R\'ef\'erences bibliographiques}

\nocite{*}
\printbibliography{}

%\`a ajouter
George A. Miller (1995). WordNet: A Lexical Database for English.
Communications of the ACM Vol. 38, No. 11: 39-41.

Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.

\appendix

\printindex


\end{document}




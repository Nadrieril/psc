
@article{horacek_building_2001,
	title = {Building natural language generation systems},
	volume = {27},
	issn = {0891-2017},
	doi = {10.1162/coli.2000.27.2.298},
	language = {English},
	number = {2},
	journal = {Computational Linguistics},
	author = {Horacek, H.},
	month = jun,
	year = {2001},
	note = {{WOS}:000169545700008},
	pages = {298--300}
}

@article{elhadad_natural_2010,
	title = {Natural Language Processing with Python},
	volume = {36},
	issn = {0891-2017},
	doi = {10.1162/coli_r_00022},
	language = {English},
	number = {4},
	journal = {Computational Linguistics},
	author = {Elhadad, Michael},
	month = dec,
	year = {2010},
	note = {{WOS}:000285382400009},
	pages = {767--771}
}

@phdthesis{parmentier_specification_1998,
	address = {Nancy},
	title = {Spécification d'une architecture émergente fondée sur le raisonnement par analogie : Application aux références bibliographiques},
	url = {http://francois.parmentier.free.fr/these/parmenti-these98.pdf},
	abstract = {{BAsCET} est un système multi-agents à "blackboard", fondé sur l'émergence de concepts
dans un modèle dynamique et inspiré de
Copycat
. Pour éviter un raisonnement déterministe
unique limitant sa créativité il adapte son comportement en fonction de la solution courante.
Nous l'avons appliqué à la reconnaissance automatique de la structure logique (des champs) de
références bibliographiques dans les articles scientifiques (en format uniquement physique, c'est-
à-dire en {PostScript}). Le modèle, appelé Réseau de Concepts, s'apparentant à la fois aux réseaux
sémantiques et aux réseaux de neurones, est construit automatiquement à partir d'une base de
références
{BibTEX}. Le système utilise les co-occurrences entre les termes des références pour
rapprocher dans le modèle ceux qui sont conceptuellement voisins. Le principe de l'analogie est
utilisé sur les références de la base: quand le système rencontre une référence inconnue, il fait
l'analogie avec la partie physique de la base et essaye de proposer une solution correspondante.
Les résultats obtenus, bien que modérés (65,5\% de reconnaissance), laissent augurer des résultats
encore meilleurs, après optimisation du système.},
	language = {Français},
	school = {Henri Poincaré},
	author = {Parmentier, François},
	month = jun,
	year = {1998}
}

@article{fattah_ga_2009,
	title = {{GA}, {MR}, {FFNN}, {PNN} and {GMM} based models for automatic text summarization},
	volume = {23},
	issn = {0885-2308},
	doi = {10.1016/j.csl.2008.04.002},
	abstract = {This work proposes an approach to address the problem of improving content selection in automatic text summarization by using some statistical tools. This approach is a trainable summarizer, which takes into account several features, including sentence position, positive keyword, negative keyword, sentence centrality, sentence resemblance to the title, sentence inclusion of name entity, sentence inclusion of numerical data, sentence relative length, Bushy path of the sentence and aggregated similarity for each sentence to generate summaries. First, we investigate the effect of cacti sentence feature on the summarization task. Then we use all features in combination to train genetic algorithm ({GA}) and mathematical regression ({MR}) models to obtain a suitable combination of feature weights. Moreover, we use all feature parameters to train feed forward neural network ({FFNN}), probabilistic neural network ({PNN}) and Gaussian mixture model ({GMM}) in order to construct a text summarizer for each model. Furthermore, we use trained models by one language to test summarization performance in the other language. The proposed approach performance is measured at several compression rates on a data corpus composed of 100 Arabic political articles and 100 English religious articles. The results of the proposed approach are promising, especially the {GMM} approach. (C) 2008 Elsevier Ltd. All rights reserved.},
	language = {English},
	number = {1},
	journal = {Computer Speech and Language},
	author = {Fattah, Mohamed Abdel and Ren, Fuji},
	month = jan,
	year = {2009},
	note = {{WOS}:000262688100007},
	keywords = {Automatic summarization, Feed forward neural network, Gaussian   mixture model, Genetic algorithm, latent semantic analysis, Mathematical regression, Probabilistic neural network, random-fields, sentence compression, speaker identification, trainable summarizer, verification},
	pages = {126--144}
}

@incollection{danlos_generation_2000,
	title = {Génération automatique de texte},
	url = {http://www.linguist.univ-paris-diderot.fr/~danlos/Dossier%20publis/GAT%2700.pdf},
	booktitle = {Ingéniérie de la langue},
	publisher = {Hermès},
	author = {Danlos, Laurence and Roussarie, Laurent},
	editor = {Pierrel, Jean-Michel},
	year = {2000}
}

@article{jones_automatic_2007,
	title = {Automatic summarising: The state of the art},
	volume = {43},
	issn = {0306-4573},
	shorttitle = {Automatic summarising},
	doi = {10.1016/j.ipm.2007.03.009},
	abstract = {This paper reviews research on automatic summarising in the last decade. This work has grown, stimulated by technology and by evaluation programmes. The paper uses several frameworks to organise the review, for summarising itself, for the factors affecting summarising, for systems, and for evaluation. The review examines the evaluation strategies applied to summarising, the issues they raise, and the major programmes. It considers the input, purpose and output factors investigated in recent summarising research, and discusses the classes of strategy, extractive and non-extractive, that have been explored, illustrating the range of systems built. The conclusions drawn are that automatic summarisation has made valuable progress, with useful applications, better evaluation, and more task understanding. But summarising systems are still poorly motivated in relation to the factors affecting them, and evaluation needs taking much further to engage with the purposes summaries are intended to serve and the contexts in which they are used. (C) 2007 Published by Elsevier Ltd.},
	language = {English},
	number = {6},
	journal = {Information Processing \& Management},
	author = {Jones, Karen Spaerck},
	month = nov,
	year = {2007},
	note = {{WOS}:000249742500004},
	keywords = {abstraction natural   language processing, abstracts, articles, evaluation, sentence extraction, summarization},
	pages = {1449--1481}
}

@article{salton_automatic_1997,
	title = {Automatic text structuring and summarization},
	volume = {33},
	issn = {0306-4573},
	doi = {10.1016/S0306-4573(96)00062-3},
	abstract = {In recent years, information retrieval techniques have been used for automatic generation of semantic hypertext links. This study applies the ideas from the automatic link generation research to attack another important problem in text processing-automatic text summarization. An automatic ''general purpose'' text summarization tool would be of immense utility in this age of information overload. Using the techniques used (by most automatic hypertext link generation algorithms) for inter-document link generation, we generate intra-document links between passages of a document. Based on the intra-document linkage pattern of a text, we characterize the structure of the text. We apply the knowledge of text structure to do automatic text summarization by passage extraction. We evaluate a set of fifty summaries generated using our techniques by comparing them to paragraph extracts constructed by humans. The automatic summarization methods perform well, especially in view of the fact that the summaries generated by two humans for the same article are surprisingly dissimilar. (C) 1997 Elsevier Science Ltd.},
	language = {English},
	number = {2},
	journal = {Information Processing \& Management},
	author = {Salton, G. and Singhal, A. and Mitra, M. and Buckley, C.},
	month = mar,
	year = {1997},
	note = {{WOS}:A1997WU98800006},
	pages = {193--207}
}

@article{ye_document_2007,
	title = {Document concept lattice for text understanding and summarization},
	volume = {43},
	issn = {0306-4573},
	doi = {10.1016/j.ipm.2007.03.010},
	abstract = {We argue that the quality of a summary can be evaluated based on how many concepts in the original document(s) that can be preserved after summarization. Here, a concept refers to an abstract or concrete entity or its action often expressed by diverse terms in text. Summary generation can thus be considered as an optimization problem of selecting a set of sentences with minimal answer loss. In this paper, we propose a document concept lattice that indexes the hierarchy of local topics tied to a set of frequent concepts and the corresponding sentences containing these topics. The local topics will specify the promising sub-spaces related to the selected concepts and sentences. Based on this lattice, the summary is an optimized selection of a set of distinct and salient local topics that lead to maximal coverage of concepts with the given number of sentences. Our summarizer based on the concept lattice has demonstrated competitive performance in Document Understanding Conference 2005 and 2006 evaluations as well as follow-on tests. (C) 2007 Elsevier Ltd. All rights reserved.},
	language = {English},
	number = {6},
	journal = {Information Processing \& Management},
	author = {Ye, Shiren and Chua, Tat-Seng and Kan, Min-Yen and Qiu, Long},
	month = nov,
	year = {2007},
	note = {{WOS}:000249742500015},
	keywords = {concept, databases, document concept lattice, knowledge discovery, semantic, text summarization},
	pages = {1643--1662}
}

@incollection{silla_automatic_2004,
	address = {Berlin},
	title = {Automatic text summarization with genetic algorithm-based attribute selection},
	volume = {3315},
	isbn = {3-540-23806-9},
	abstract = {The task of automatic text summarization consists of generating a summary of the original text that allows the user to obtain the main pieces of information available in that text, but with a much shorter reading time. This is an increasingly important task in the current era of information overload. given the huge amount of text available in documents. In this paper the automatic text summarization is cast as a classification (supervised learning) problem, so that machine learning-oriented classification methods are used to produce summaries for documents based on a set of attributes describing those documents. The goal of the paper is to investigate the effectiveness of Genetic Algorithm ({GA})-based attribute selection in improving the performance of classification algorithms solving the automatic text summarization task. Computational results are reported for experiments with a document base formed by news extracted from The Wall Street Journal of the {TIPSTER} collection-a collection that is often used as a benchmark in the text summarization literature.},
	language = {English},
	booktitle = {Advances in Artificial Intelligence - Iberamia 2004},
	publisher = {Springer-Verlag Berlin},
	author = {Silla, C. N. and Pappa, G. L. and Freitas, A. A. and Kaestner, C. a. A.},
	editor = {Lemaitre, C. and Reyes, C. A. and Gonzalez, J. A.},
	year = {2004},
	note = {{WOS}:000226646200031},
	pages = {305--314}
}


@article{liu_conceptnet_2004,
	title = {{ConceptNet} - a practical commonsense reasoning tool-kit},
	volume = {22},
	issn = {1358-3948},
	doi = {10.1023/B:BTTJ.0000047600.45421.6d},
	abstract = {{ConceptNet} is a freely available commonsense knowledge base and natural-language-processing tool-kit which supports many practical textual-reasoning task over real-world documents including topic-gisting, analogy-making, and other context oriented inferences. The knowledge base is a semantic network presently consisting of over 1.6 million assertions of commonsense knowledge encompassing the spatial, physical, social, temporal, and psychological aspects of everyday life. {ConceptNet} is generated automatically from the 700 000 sentences of the Open Mind common Sense Project-a World Wide Web based collaboration with over 14 000 authors.},
	language = {English},
	number = {4},
	journal = {Bt Technology Journal},
	author = {Liu, H. and Singh, P.},
	month = oct,
	year = {2004},
	note = {{WOS}:000224961900027},
	pages = {211--226}
}
 
 @article{Miller95wordnet:a,
    author = {George A. Miller},
    title = {WordNet: A Lexical Database for English},
    journal = {COMMUNICATIONS OF THE ACM},
    year = {1995},
    volume = {38},
    pages = {39--41}
}


@article{Miller90wordnet:an,
    author = {George A. Miller and Richard Beckwith and Christiane Fellbaum and Derek Gross and Katherine Miller},
    title = {WordNet: An on-line lexical database},
    journal = {International Journal of Lexicography},
    year = {1990},
    volume = {3},
    pages = {235--244}
}


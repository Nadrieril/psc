\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}%%seul package à charger~: pour éviter les problèmes sordides d'encodage
\usepackage[rapport, psc]{andre}%%bon eh bien sûr...

\usepackage{makeidx}              %% permet de générer un index automatiquement
\usepackage[style=numeric, backend=bibtex]{biblatex}				%% Utilisé pour la biblio



\title{Projet Scientifique Collectif X2013 \\INF02~: Synthétiseur automatique de documents} 
\renewcommand{\petittitre}{Projet INF02}
\newcommand{\pyt}[1]{\texttt{#1}}%nooms python
\newcommand{\ang}[1]{\textit{#1}}%noms anglais
\author{\membres} %%\membres uniquement avec l'option psc
\date{21 janvier 2015}

%mettre les accents comme \c{c}a, sinon risque de plantage
\index{Réseau sémantique}
\index{Analyse symbolique}
\index{Analyse sémantique}
\index{Réseau de Concepts}

\makeindex
\addbibresource{biblio.bib}
\begin{document}

\titrelong{}


\tableofcontents			
\newpage

\section{Rappel du projet}

\subsection{Notre groupe}
\begin{itemize}
 \item Fernandes-Pinto-Fachada Sarah, \textbf{8\textsuperscript{e}} compagnie, section \textbf{équitation};
 \item Schrottenloher André, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Angibault Antonin, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Hufschmitt Théophane, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Cao Zhixing, \textbf{9\textsuperscript{e}} compagnie, section \textbf{escalade};
 \item Boisseau Guillaume, \textbf{6\textsuperscript{e}} compagnie, section \textbf{natation};
\end{itemize}

\subsection{Notre sujet}

\subsubsection{But du projet}

Le but de notre sujet est de produire un programme capable d'analyser sémantiquement\index{Analyse sémantique} un texte ayant pour sujet le sport (et plus particulièrement encore le football) de façon à en produire un résumé. Il s'agit là d'une véritable innovation par rapport à l'état actuel du résumé automatique, dans la mesure où les programmes existants se basent prioritairement sur une analyse statistique plus ou moins poussée de fréquence d'apparition des mots pour juger de leur importance~; notre programme se baserait plutôt sur le sens pour juger de la pertinence d'un concept et savoir s'il doit le faire figurer ou non dans le résumé final.

Dans l'état actuel des choses, nous n'espérons pas produire un programme utilisable dans l'industrie ou plus efficace que ceux déjà existants~; nous pensons en revanche que cette autre manière d'aborder le résumé automatique est plus viable, dans la mesure où elle analyse plus directement que par la méthode statistique.

\subsubsection{Modules du projet et répartition du travail}
Pour atteindre cet objectif, plusieurs modules assez indépendants ont pu être identifiés et traités séparément~:

\begin{description}
	\item[Récupération de données] à utiliser pour la création d'un réseau de concepts modélisant la connaissance \textit{a priori} du programme. Responsables~: Théophane Hufschmitt, Guillaume Boisseau.
	\item[Création du réseau de concepts] et remplissage de celui-ci. Il s'agissait ici d'une part de définir sa structure et d'autre part d'y introduire les données nécessaires. Responsables~: André Schrottenloher, Guillaume Boisseau.
	\item[Analyse syntaxique] à l'aide d'outil existants. Il s'agissait de trouver une grammaire capable d'analyser une phrase et d'en faire ressortir les relations de type verbe-objet ou nom-qualificatif. Responsables~: Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada
	\item[Réflexion] sur l'utilisation du réseau de concepts et de l'analyse grammaticale pour produire un réseau capable de rendre compte de la compréhension du texte. Responsables~: tous.
	\item[Implémentation] d'un algorithme simple de résumé stastique (basé sur l'algorithme TF-IDF), devant servir de base de comparaison pour tester la qualité des résumés produits par notre algorithme.  Responsable~: Zhixing Cao. 
\end{description}

\subsubsection{Outils extérieurs utilisés}

Le volume de travail représenté par la création et l'utilisation du réseau de concepts\index{Réseau de concepts} est tel que nous nous appuyons sur un certain nombre d'outils pour traiter le texte en amont de notre résumé. La construction du réseau s'appuie, elle aussi, sur des bases de connaissances déjà existantes (et libres) dont voici la liste~:

\begin{itemize}
	\item[WordNet~: ]Dictionnaire permettant d'obtenir la classe grammaticale des mots présents dans le réseau.
	\item[Conceptnet~: ]Grande base de connaissances permettant d'établir la proximité conceptuelle entre différents termes du réseau.
	\item[Freebase~: ]Cette base de connaissance permet d'identifier les noms propres, tels que les joueurs de foot ou les différents stades.
	\item[Natural Language ToolKit (\pyt{nltk})~: ]Divers outils grammaticaux pour l'étude du langage naturel.
\end{itemize}

\section{\'Etat d'avancement~: récupération de données}

Nous récupérons des données sur différents sites internet et flux rss, en moyenne 400 articles de sports par jour.

Ces données vont servir à différents niveaux~:
\begin{itemize}
 \item Dans la création du réseau de concepts (partie suivante)~;
 \item Dans la production de résumés automatiques.
\end{itemize}


\section{Le réseau de concepts}



\subsection{Structure}


\begin{definition}[Réseau de concepts]
Le réseau de concepts\index{Réseau de concepts} est la ``mémoire à long terme'' de notre programme.

Ce réseau est au carrefour entre les réseaux sémantiques et les réseaux de neurones. Les n\oe{}uds du réseau ne représentent pas des concepts à proprement parler~; en revanche, les concepts sont vus comme regroupant des n\oe{}uds fortement interconnectés.
\end{definition}


Le Réseau est constitué de n\oe{}uds. Chaque n\oe{}ud comporte~:
\begin{itemize}
  \item Une étiquette (mot)~;
 \item Une importance conceptuelle (IC) ou profondeur conceptuelle~;
 \item Une activation (A) initialement nulle~;
 \item Un certain nombre de liens à d'autres n\oe{}uds.
\end{itemize}

Chaque lien comporte~:
\begin{itemize}
 \item Une proximité conceptuelle (ou poids W)~;
 \item Une étiquette qui le caractérise.
\end{itemize}

\subsection{Outils pour la construction}

Le but est donc de créer un réseau de concepts de base, que nous puissions utiliser comme base de connaissances dans le domaine considéré (ici le sport, mais nous pourrions avoir sélectionné un autre domaine).

Nous utilisons deux types de sources~:
\begin{itemize}
 \item Des articles de sport (données brutes) qui contiennent entre autres ``ce qu'il faut savoir''~;
 \item Des données sémantiques libres.
\end{itemize}


\subsubsection{Traitement de données textuelles}

Les concepts que nous insérerons dans le réseau seront des mots sous forme normale.

\begin{definition}[Forme normale]
La forme normale d'un mot\index{Forme normale} est tout simplement celle qu'il a lorsqu'on le recherche dans le dictionnaire, par exemple~:
\begin{itemize}
 \item Si c'est un verbe, on le met à l'infinitif~;
 \item Au singulier si c'est un nom.
\end{itemize}
Mettre un mot sous forme normale permet de s'intéresser uniquement au sens du mot hors de tout contexte.
\end{definition}


\begin{definition}[Part-of-speech tagger]
Le Part-of-speech tagger\index{Part-of-speech tagger}, abrégé en POS, est un programme, le plus souvent entraîné sur de grandes bases de données, qui permet d'associer à chaque mot dans une phrase une valeur grammaticale.
\end{definition}

\`A partir d'articles consacrés au sport (texte brut), les manipulations suivantes permettent de récupérer les noms propres et les concepts sous forme normale~:
\begin{itemize}
 \item Découpage du texte en phrases et en mots (\textit{tokens})~;
 \item Utilisation d'un POS, celui de \pyt{nltk}\index{Natural Language ToolKit} par défaut~;
 \item Récupération des noms propres et mise à part de ceux-ci (qui doivent faire plus tard l'objet d'un traitement particulier)~;
 \item Suppression des conjonctions de coordination, pronoms et autres mots qui ne sont pas de véritables concepts~;
 \item Il reste alors des adjectifs, adverbes, noms et verbes. Nous utilisons \pyt{morphy} de WordNet\index{WordNet} à l'intérieur d'un code dédié à la construction de Conceptnet5 (voir partie suivante), pour transformer chaque terme en sa forme normale~;
 \item Les termes restants sont alors tous les concepts auxquels le texte fait appel~;
 \item Nous ne sommes pas à l'abri d'erreurs liées à l'une des étapes. De manière générale, un concept qui apparaît au moins deux fois dans l'ensemble du corpus considéré peut être considéré comme valide.
\end{itemize}


\subsubsection{WordNet}

WordNet est une base de données lexicale pour l'anglais, produite par l'université de Princeton.

Nous l'utilisons à travers son interface \pyt{nltk}, pour récupérer des informations sur les mots et les mettre sous forme normale.


\subsubsection{Conceptnet5}

Conceptnet (<http://conceptnet5.media.mit.edu/>) est un projet libre de réseau sémantique représentant des connaissances usuelles, aussi bien de la vie de tous les jours que culturelles et scientifiques. Il fait partie du \ang{Commonsense Computing Initiative} qui relie différents laboratoires, dont le MIT Media Lab, et entreprises.

Il est important de noter que Conceptnet5 est généré à partir de données brutes. Conceptnet5 est relié à DBPedia, une grande partie de ses connaissances provient de Wiktionary, une partie de WordNet.

Nous avons utilisé une partie de code source provenant de la page <https://github.com/commonsense/conceptnet5>.


\subsubsection{Structure de Conceptnet5}

Conceptnet5\index{Conceptnet5} est un graphe sémantique contenant de l'ordre du million de nœuds et d'ar\^etes.

Les n\oe{}uds sont des mots et de courtes phrases, pas seulement en anglais. Les arêtes qui relient ces n\oe{}uds expriment une connaissance, elles contiennent chacune une relation particulière.

Une arête possède aussi une source (d'où provient l'information) et un poids en fonction de cette source, selon l'importance de l'arête.

Une relation concept-arête-concept exprime une assertion\index{Assertion}. Une même assertion peut être exprimée de différentes manières. Conceptnet5 contient 1.6 million d'assertions.

Une assertion peut elle-même être utilisée comme un n\oe{}ud ou comme une arête (on peut avoir des assertions d'assertions).

Les relations valables dans tout langage, pour Conceptnet5, sont par exemple~: 
\begin{itemize}
 \item RelatedTo~;
 \item IsA~;
 \item PartOf~;
 \item MemberOf~;
 \item HasA~;
 \item TranslationOf~;
 \item DefinedAs\ldots{}
\end{itemize}

Il en existe beaucoup d'autres.

Comme Conceptnet est construit en partie à l'aide de WordNet, il existe une correspondance entre relations de l'un et de l'autre~:

La correspondance entre relations de WordNet et de Conceptnet est par exemple~:
\begin{itemize}
 \item `attribute': `Attribute'~;
 \item `causes': `Causes'~;
 \item `classifiedByRegion': `HasContext'~;
 \item `hyponymOf': `IsA'~;
 \item `sameVerbGroupAs': `SimilarTo'~;
 \item `seeAlso': `RelatedTo'.
\end{itemize}

Nous avons décidé après coup de conserver ces relations dans le réseau de concepts, et d'en utiliser un sous-ensemble.

\subsubsection{Détail de l'API}

Nous pouvons faire un certain nombre de requêtes à Conceptnet5. Il existe trois types différents de requêtes~:
\begin{itemize}
 \item Lookup~;
 \item Association~;
 \item Search~;
\end{itemize}
Association permet de calculer la proximité entre deux concepts, ou de récupérer une liste de concepts proches d'un concept donné.

Search permet de récupérer une liste d'arêtes (\ang{edges}) entre concepts, selon les paramètres spécifiés (le plus souvent, on impose le concept de départ \ang{start} ou d'arrivée \ang{end}). 

Lookup permet d'analyser un concept (on aura par exemple accès à des listes d'arêtes dans lequel il intervient).


\subsubsection{Utilisation de Conceptnet5}

Nous utilisons en majorité Association et Search.

Étant donné un concept dont nous savons qu'il doit être étendu, nous utilisons conceptnet5 pour créer de nouveaux concepts au sein du réseau, et pour ajouter de nouveaux liens. Une première requête de type Search permet d'avoir accès à un certain nombre de liens vers d'autres concepts, qui sont ajoutés (on ne récupère que les plus pertinents). Une requête de type Association permet de créer de nouveaux liens vers d'autres concepts similaires.

Nous ajoutons alors une arête SimilarTo.
    
\subsubsection{Freebase}

Freebase\index{Freebase} est une immense base de données sémantiques qui contient beaucoup d'informations, sur des noms propres notamment. Le projet, repris par google mais sous une license qui laisse les données libres d'accès, de téléchargement et d'utilisation, a lui aussi une API, un peu plus complexe que celle de Conceptnet5. Freebase permet notamment de récupérer des informations sur une personne à partir de son nom, et à peu de frais, de vérifier que cette personne existe bel et bien.


\subsection{Construction du réseau et résultats}

\`A l'aide des outils susmentionnés, nous avons étés à même de construire un premier réseau de concepts.
Nous présentons dans cette partie les caractéristiques du réseau construit, ainsi que les choix de programmation effectués.

\subsubsection{Récupération de concepts et de noms}

Nous analysons un certain nombre d'articles, ici 3043 issus 7 flux rss différents sur 16 jours.

Nous utilisons en partie les \ang{tokenizers} de \pyt{nltk}, et nous complétons si besoin.

Pour récupérer les noms propres, nous repérons à l'intérieur d'un article les mots marqués comme noms propres par le POS de \pyt{nltk}. Puis nous concaténons deux noms propres successifs. Enfin, nous retirons de la liste des noms propres ceux qui sont contenus à l'intérieur d'autres. Cette étape, largement sous-estimée au début, est cruciale puisqu'elle évite de considérer ``< nom + prénom >'' et ``<prénom>'' comme deux personnes différentes.

Nous comptons aussi la fréquence d'apparition de chaque concept. On obtient 9744 noms et environ 12700 concepts, ce qui est très raisonnable.

\subsubsection{Traitement des noms avec Freebase}

Nous n'utilisons pour l'instant l'API Freebase que de fa\c{c}on très superficielle. Chaque nom propre donne lieu à une requête. Si celle-ci aboutit, on analyse le résultat et si celui-ci comporte une donnée (exemple~: à l'envoi de \verb|"scott_jamieson"|, on récupère \verb|"soccer_midfielder"|), on ajoute un nouveau nœud correspondant au réseau, et une arête ``<IsA>'' dont le poids dépend du résultat de la requête.

Dans notre exemple, 676 noms seront rejetés par Freebase~: c'étaient donc des résultats erronés de la première étape.

Environ 7500 nœuds sont créés dans le réseau.

\subsubsection{Traitement des concepts avec Conceptnet5}

Pour chaque concept dans la liste, on envoie deux requêtes~: l'une vise à trouver des arêtes qui partent de lui (Search), l'autre à trouver des concepts proches (Association). 

En l'absence de résultat, le concept ne sera pas ajouté au réseau.

Dans le cas contraire, de nouvelles arêtes sont créées. Leurs poids dépendent des résultats des requêtes. Pour la requête Association, nous mettons une relation ``<SimilarTo>''.

L'importance conceptuelle d'un nouveau nœud dépend essentiellement de son nombre d'apparitions dans les articles. D'autres possibilités sont envisageables (nombre de synonymes avec WordNet, par exemple).

Dans notre exemple, sur les 12700 concepts, 4800 ne donnent lieu à aucun résultat. Cela peut paraître beaucoup~; en réalité, quand on regarde de plus près la ``<liste noire>'' (que nous stockons), on a par exemple~:``reimagining'',``money-grabbers'',``whoscored'',``impeccable'',``amateurish'',``dawkins'',``rat-a-tat'',``single-handedly'',``petered'',``real-politik'',``re-tweet'',``throwing''

Ces mots sont soit composés (et donc difficiles à analyser), soit résultent d'erreurs du POS-tagger (on voit passer quelques noms propres).

Cette liste pourra bien sûr donner lieu à une analyse plus fine (nouvelles requêtes Freebase).

\`A ce stade, notre réseau possède 24000 nœuds environ, ce qui reste modique.

\subsubsection{Extension des nœuds du réseau}

Nous envisageons maintenant, par de nouvelles requêtes, d'étendre les nœuds que nous venons de créer. Le but est que le réseau soit au mieux interconnecté, or de nombreux nœuds résultant des requêtes précédentes ne pointent vers aucun nœud.

D'autres opérations sont possibles.

Nous regardons 5000 de ces nœuds faiblement connectés. Après requêtes, 2300 sont reliés au réseau. (Cette étape a été particulièrement lente).

\subsubsection{Suppression des nœuds inutiles}

Malgré les nombreux paramètres servant à filtrer, les requêtes n'ont pas donné des résultats toujours pertinents. Nous choisissons de retirer du réseau les nœuds faiblement reliés (un seul voisin). Ceux-ci seront de toute fa\c{c}on peu utiles lors de la propagation de l'activation.

\`A  ce stade, le réseau comporte 24000 nœuds et 35000 arêtes. Après nettoyage, il reste 17500 nœuds et 29000 arêtes.

Nous disposons maintenant d'un réseau utilisable (en théorie).

\subsubsection{Premiers tests avec ce réseau}

Pour chaque test, nous activons un nœud et laissons l'activation se propager à ses successeurs au cours du temps (sans désactivation). \`A chaque étape (nouvel instant) se produit un calcul d'activation.

\begin{itemize}
 \item \verb|"manchester", 60|
 \begin{verbatim}
  [...]
  Time=10
  comic_book_character~: 1~; furniture : 4
  house~: 5~; manchester : 60
  object~: 9~; place : 11
  thing~: 6~; desk : 4
  location~: 21~; loft : 3
 \end{verbatim}
\item \verb|"francis_lastic", 60|
\begin{verbatim}
  [...]
  Time=5
  coach~: 23~; francis_lastic : 60
\end{verbatim}
\item \verb|"ball_sport", 60|
\begin{verbatim}
  [...]
  Time=10
  ball_sport~: 60~; team_sport : 40
  basketball~: 40~; interest : 7
  football~: 40
\end{verbatim}
\end{itemize}

\subsection{Choix de programmation et prospectives}

Nous avons d'abord cru que l'utilisation massive de requêtes donnerait des contraintes de temps importantes à nos programmes. C'est en réalité une erreur. La parallélisation des requêtes, que nous avons commencé à effectuer, permet un gain de temps exceptionnel et va nous conduire dans les prochaines semaines à améliorer considérablement le temps d'exécution.

Le réseau est pour le moment enregistré sous forme de JSON Stream, un type de données repris à Conceptnet5. Le nombre de nœuds étant resté raisonnable, nous n'avons pas eu à abandonner prématurément le package \pyt{networkx} qui nous permet de le représenter en mémoire.

Cependant, nous envisageons de passer à une base de données.

Enfin, nous nous orientons maintenant vers des fonctions permettant de construire incrémentalement le réseau, donc d'ajouter les nouveaux concepts et noms propres au fur et à mesure qu'ils sont lus dans les articles (notre but initial était naturellement de construire un réseau ``<de base>'').

\section{Traitement du réseau}

Le réseau de concepts représente, comme nous l'avons déjà dit, la connaissance \textit{a priori} de notre programme sur le domaine étudié. Pour étudier un texte particulier, il s'agit maintenant de faire agir ce texte sur nos connaissances et d'interpréter l'effet que cette lecture a.

Deux méthodes ont été envisagées~: la première étudiait, essentiellement, la différence entre l'état du réseau de concepts avant et après la lecture et devait en déduire les faits importants. La seconde, qui a finalement été retenue, s'appuie sur une activation des concepts dans le réseau, puis une instanciation des concepts fortement activés dans un espace de travail appelé \textit{workspace}\index{Workspace}. Ces concepts instanciés sont par la suite capable d'effectuer des tâches plus complexes relatives à la compréhension du texte.

Quelle que soit la méthode employée, une analyse syntaxique\index{Analyse syntaxique} préalable du texte est nécessaire pour rendre compte de sa compréhension et utiliser au mieux notre réseau de connaissance.

\subsection{Workspace}\index{Workspace}



\subsection{Analyse syntaxique}\index{Analyse syntaxique}

Cette partie ne constituant pas le c\oe{}r de notre psc, nous avons décidé d'utiliser un outil déjà codé, faisant partie de la plus grande librairie libre \pyt{nltk}\index{Natural Language ToolKit}.

\subsubsection{Description algorithmique d'une grammaire}\index{Grammaire}
Une grammairep peut être décrite informatiquement de manière très simple. En effet, on peut la voir comme~:
\begin{itemize}
	\item Un ensemble de classes terminales (verbe, nom par exemple) associées à une liste de mots ou éventuellement d'expressions
	\item Un ensemble de règles, souvent sous la forme d'une reconnaissance de motifs, permettant de découper une phrase en plusieurs éléments (et finalement en unités syntaxiques élémentaires).
\end{itemize}

Il y a par conséquent deux manières pour une grammaire d'être incomplète~: soit elle ne connaît pas assez de vocabulaire (manque d'éléments dans les cas terminaux), soit elle ne connaît pas assez de règles. La première faille est assez facile à combler par des requêtes vers WordNet\index{WordNet} (et par ailleurs un outil automatique est inclus dans \pyt{nltk}\index{Natural Language ToolKit})~; il est en revanche assez indispensable d'avoir une grammaire complète en termes de règles car il est beaucoup plus difficile d'inventer des manières de découper une phrase.

\subsubsection{\'Etat du texte en fin d'analyse}
Les grammaires nltk sont classées selon la forme de leur sortie. Elles attendent la liste des mots dans l'ordre dans lequel ils apparaissent dans le texte à étudier, accompagnés d'une liste de classes possibles pour ces mots. En sortie, on peut trouver~:

\begin{itemize}
	\item Un arbre représentant une structure grammaticale possible pour la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase, et pour chaque arbre une mesure de la vraisemblance de cette structure.
\end{itemize} 

Il y a une grande différence entre les n\oe{}uds non terminaux des arbres en sortie et leurs feuilles. En effet, ces dernières seront les unités syntaxiques présentes à l'origine dans la phrase, tandis que les n\oe{}uds internes représentent la classe grammaticale des bouts de phrases représentés par les sous-arbres dont ils sont la racine.

%Un exemple serait sûrement le bienvenu, je vais voir si je me chauffe pour faire un ParseTree a l'air plausible. (Antonin)
%Cela étant n'hésitez pas à vous chauffer vous aussi.

Les n\oe{}uds internes sont en nombre fini et leurs étiquettes dépend de si on considère des grammaires sans contexte (capables de séparer des composants autour d'un verbe) ou des grammaires à dépendance (dont le but est d'identifier le sujet et les compléments du verbe central).


\section{Suites envisagées}

Dans l'ensemble le projet avance avec peu de retard sur le plan prévisionnel écrit en novembre dernier. Le réseau de concepts est prêt à être utilisé et notre attention pourra se reporter sur le développement du \textit{workspace}\index{Workspace} et du déploiement de l'analyse syntaxique.

Cette dernière, que nous avions prise pour à peu près acquise, s'avère requérir plus de travail que prévu (il nous faudra l'entraîner et la préparer un peu, ce qui requiert une certaine compréhension des mécanismes qui la sous-tendent).

La planification pour le reste du projet est la suivante~:
\begin{description}
	\item[Février~: ]Mise en place d'une grammaire \pyt{nltk}. Responsables~: Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada.
	\item[Février~: ]Fin de la spécification du contenu du \textit{workspace}.
	\item[Février-Avril~: ]Implémentation du \textit{workspace} (donc, essentiellement, de ce qui manque à l'algorithme de résumé pour fonctionner).
	\item[Avril~: ]Génération de résumé et évaluation de ceux-ci.
\end{description}

Les différences essentielles avec la planification initiale proviennent de la complexité inattendue de la grammaire. En revanche, le temps d'exécution prévu pour la production d'un résumé d'article étant long, nous produirons finalement peu de résumés à évaluer, cela nous permet donc de les évaluer à la main et de gagner du temps sur cette partie de la planification initiale.

Globalement nous pensons être capables de mener le projet à bien comme prévu. Les sections suivantes décrivent les points les plus prêts d'être mis en place.

\subsection{Apprentissage du réseau de concepts}
Le réseau de concept représente un \textit{a priori} sur le monde de notre programme. Il est donc normal que cette structure évolue au fur et à mesure des textes lus. 

Les fonctions de construction du réseau peuvent sans mal être appelées après la génération du résumé pour y apporter des modifications pérennes.

\subsection{\'Evaluation des résumés}
Le faible nombre de résumés que nous prévoyons de produire nous permettra d'utiliser la méthode la plus simple parmi celles qui étaient mentionnées dans la proposition détaillée~: la comparaison à un résumé déclaré bon car produit par un humain.

L'idée est la suivante~: l'un de nous résumera un texte, nous emploierons notre programme pour résumer le même texte, et enfin nous utiliserons un simple algorithme statistique (basé sur TF-IDF) pour résumer ce même texte. Nous comparerons ensuite ces résumés du point de vue de l'information qu'ils contiennent~: sont-ils des images assez fidèles du document original~?

Compte-tenu de la simplicité de l'algorithme statistique, nous espérons que le résumé produit par notre programme se situera quelque part entre le résumé statistique et le résumé humain.

\section{Références bibliographiques}

\nocite{*}
\printbibliography{}

\appendix

\printindex


\end{document}




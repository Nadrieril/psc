\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}%%seul package à charger~: pour éviter les problèmes sordides d'encodage
\usepackage[rapport, psc]{andre}%%bon eh bien sûr...

\usepackage{makeidx}              %% permet de générer un index automatiquement
\usepackage[style=numeric, backend=bibtex]{biblatex}				%% Utilisé pour la biblio
\usepackage[hidelinks]{hyperref}



\title{Projet Scientifique Collectif X2013 \\INF02~: Synthétiseur automatique de documents}
\renewcommand{\petittitre}{Projet INF02}
\newcommand{\pyt}[1]{\texttt{#1}}%nooms python
\newcommand{\ang}[1]{\textit{#1}}%noms anglais
\author{\membres} %%\membres uniquement avec l'option psc
\date{21 janvier 2015}

%mettre les accents comme \c{c}a, sinon risque de plantage
\index{Réseau sémantique}
\index{Analyse symbolique}
\index{Analyse sémantique}
\index{Réseau de Concepts}

\makeindex
\addbibresource{biblio.bib}
\begin{document}

\titrelong{}


\tableofcontents
\newpage

\section{Rappel du projet}

\subsection{Notre groupe}
\begin{itemize}
 \item Fernandes-Pinto-Fachada Sarah, \textbf{8\textsuperscript{e}} compagnie, section \textbf{équitation};
 \item Schrottenloher André, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Angibault Antonin, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Hufschmitt Théophane, \textbf{8\textsuperscript{e}} compagnie, section \textbf{escrime};
 \item Cao Zhixing, \textbf{9\textsuperscript{e}} compagnie, section \textbf{escalade};
 \item Boisseau Guillaume, \textbf{6\textsuperscript{e}} compagnie, section \textbf{natation};
\end{itemize}

\subsection{Notre sujet}

\subsubsection{But du projet}

Le but de notre sujet est de produire un programme capable d'analyser sémantiquement\index{Analyse sémantique} un texte ayant pour sujet le sport (et plus particulièrement encore le football) de façon à en produire un résumé. Il s'agit là d'une véritable innovation par rapport à l'état actuel du résumé automatique, dans la mesure où les programmes existants se basent prioritairement sur une analyse statistique plus ou moins poussée de fréquence d'apparition des mots pour juger de leur importance~; notre programme se baserait plutôt sur le sens pour juger de la pertinence d'un concept et savoir s'il doit le faire figurer ou non dans le résumé final.

Dans l'état actuel des choses, nous n'espérons pas produire un programme utilisable dans l'industrie ou plus efficace que ceux déjà existants~; nous pensons en revanche que cette autre manière d'aborder le résumé automatique est plus viable, dans la mesure où elle analyse plus directement que par la méthode statistique.

\subsubsection{Modules du projet et répartition du travail}
Pour atteindre cet objectif, plusieurs modules assez indépendants ont pu être identifiés et traités séparément~:

\begin{description}
	\item[Récupération de données] à utiliser pour la création d'un réseau de concepts modélisant la connaissance \textit{a priori} du programme. Responsables~: Théophane Hufschmitt, Guillaume Boisseau.
	\item[Création du réseau de concepts] et remplissage de celui-ci. Il s'agissait ici d'une part de définir sa structure et d'autre part d'y introduire les données nécessaires. Responsables~: André Schrottenloher, Guillaume Boisseau.
	\item[Analyse syntaxique] à l'aide d'outil existants. Il s'agissait de trouver une grammaire capable d'analyser une phrase et d'en faire ressortir les relations de type verbe-objet ou nom-qualificatif. Responsables~: Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada
	\item[Réflexion] sur l'utilisation du réseau de concepts et de l'analyse grammaticale pour produire un réseau capable de rendre compte de la compréhension du texte. Responsables~: tous.
	\item[Implémentation] d'un algorithme simple de résumé stastique (basé sur l'algorithme TF-IDF), devant servir de base de comparaison pour tester la qualité des résumés produits par notre algorithme.  Responsable~: Zhixing Cao.
\end{description}

\subsubsection{Outils extérieurs utilisés}

Le volume de travail représenté par la création et l'utilisation du réseau de concepts\index{Réseau de concepts} est tel que nous nous appuyons sur un certain nombre d'outils pour traiter le texte en amont de notre résumé. La construction du réseau s'appuie, elle aussi, sur des bases de connaissances déjà existantes (et libres) dont voici la liste (ils seront traités dans la section suivante)~:

\begin{itemize}
	\item[WordNet~: ]Dictionnaire permettant d'obtenir la classe grammaticale des mots présents dans le réseau.
	\item[Conceptnet~: ]Grande base de connaissances permettant d'établir la proximité conceptuelle entre différents termes du réseau.
	\item[Freebase~: ]Cette base de connaissances permet d'identifier les noms propres, tels que les joueurs de foot ou les différents stades.
	\item[Natural Language ToolKit (\pyt{nltk})~: ]Divers outils grammaticaux pour l'étude du langage naturel.
\end{itemize}



\section{Avancement du projet}

Le groupe a essentiellement travaillé sur la récupération de données pour la création du réseau de concepts et l'utilisation de TF-IDF, ainsi que sur la mise en place de ce réseau. Cette section décrit d'une part les sources de données que nous avons employées, et d'autre part la structure et la construction du réseau de concepts.

\subsection{Sources de données}

Nous utilisons deux sources de données~: des données brutes, sous forme de langage naturel (textes d'articles écrits en anglais), et des données déjà travaillées, le plus souvent mises sous la forme de graphe sémantique.\index{Graphe sémantique}

\begin{definition}[Graphe sémantique]
En première approximation, nous faisons renvoyer la notion de graphe sémantique à un graphe où les nœuds sont étiquetés par des mots et où les arêtes représentent un lien ou expriment une relation entre ces mots.
\end{definition}


\subsubsection{Données textuelles brutes}

Nous récupérons automatiquement des données sur différents sites internet et flux rss, en moyenne 400 articles de sports par jour.

Ces données vont servir à différents niveaux~:
\begin{itemize}
 \item Dans la création du réseau de concepts (partie suivante)~;
 \item Dans la production de résumés automatiques.
\end{itemize}



\subsubsection{Traitement de données textuelles}

Les mots présents dans les articles, pour être reliés à leur signification, doivent être mis sous forme normale.

\begin{definition}[Forme normale]
La forme normale d'un mot\index{Forme normale} est tout simplement celle qu'il a lorsqu'on le recherche dans le dictionnaire, par exemple~:
\begin{itemize}
 \item Si c'est un verbe, on le met à l'infinitif~;
 \item Au singulier si c'est un nom.
\end{itemize}
Mettre un mot sous forme normale permet de s'intéresser uniquement au sens du mot hors de tout contexte.
\end{definition}


\begin{definition}[Part-of-speech tagger]
Le Part-of-speech tagger\index{Part-of-speech tagger}, abrégé en POS, est un programme, le plus souvent entraîné sur de grandes bases de données, qui permet d'associer à chaque mot dans une phrase une valeur grammaticale.
\end{definition}

\`A partir d'articles consacrés au sport (texte brut), les manipulations suivantes permettent de récupérer les noms propres et les concepts sous forme normale~:
\begin{itemize}
 \item Découpage du texte en phrases et en mots (\textit{tokens})~;
 \item Utilisation d'un POS, celui de \pyt{nltk}\index{Natural Language ToolKit} par défaut~;
 \item Récupération des noms propres et mise à part de ceux-ci (qui doivent faire plus tard l'objet d'un traitement particulier)~;
 \item Suppression des conjonctions de coordination, pronoms et autres mots qui ne sont pas de véritables concepts~;
 \item Il reste alors des adjectifs, adverbes, noms et verbes. Nous utilisons \pyt{morphy} de WordNet\index{WordNet} à l'intérieur d'un code dédié à la construction de Conceptnet5 (voir partie suivante), pour transformer chaque terme en sa forme normale~;
 \item Les termes restants sont alors tous les concepts auxquels le texte fait appel~;
 \item Nous ne sommes pas à l'abri d'erreurs liées à l'une des étapes. De manière générale, un concept qui apparaît au moins deux fois dans l'ensemble du corpus considéré peut être considéré comme valide.
\end{itemize}


\subsubsection{WordNet}

WordNet est une base de données lexicale pour l'anglais, produite par l'université de Princeton.

Nous l'utilisons à travers son interface \pyt{nltk}, pour récupérer des informations sur les mots et les mettre sous forme normale.


\subsubsection{Conceptnet5}

Conceptnet (\href{http://conceptnet5.media.mit.edu/}{leur site}) est un projet libre de réseau sémantique représentant des connaissances usuelles, aussi bien de la vie de tous les jours que culturelles et scientifiques. Il fait partie du \ang{Commonsense Computing Initiative} qui relie différents laboratoires, dont le MIT Media Lab, et entreprises.

Il est important de noter que Conceptnet5 est généré à partir de données brutes. Conceptnet5 est relié à DBPedia, une grande partie de ses connaissances provient de Wiktionary, une partie de WordNet.

Nous avons utilisé une partie de code source provenant de \href{https://github.com/commonsense/conceptnet5}{<github>}.


\subsubsection{Structure de Conceptnet5}

Conceptnet5\index{Conceptnet5} est un graphe sémantique contenant de l'ordre du million de nœuds et d'ar\^etes.

Les n\oe{}uds sont des mots et de courtes phrases, pas seulement en anglais. Les arêtes qui relient ces n\oe{}uds expriment une connaissance, elles contiennent chacune une relation particulière.

Une arête possède aussi une source (d'où provient l'information) et un poids en fonction de cette source, selon l'importance de l'arête.

Une relation concept-arête-concept exprime une assertion\index{Assertion}. Une même assertion peut être exprimée de différentes manières. Conceptnet5 contient 1.6 million d'assertions.

Une assertion peut elle-même être utilisée comme un n\oe{}ud ou comme une arête (on peut avoir des assertions d'assertions).

Les relations valables dans tout langage, pour Conceptnet5, sont par exemple~:
\begin{itemize}
 \item RelatedTo~;
 \item IsA~;
 \item PartOf~;
 \item MemberOf~;
 \item HasA~;
 \item TranslationOf~;
 \item DefinedAs\ldots{}
\end{itemize}

Il en existe beaucoup d'autres.

Comme Conceptnet est construit en partie à l'aide de WordNet, il existe une correspondance entre relations de l'un et de l'autre~:

La correspondance entre relations de WordNet et de Conceptnet est par exemple~:
\begin{itemize}
 \item attribute~: Attribute~;
 \item causes~: Causes~;
 \item classifiedByRegion~: HasContext~;
 \item hyponymOf~: IsA~;
 \item sameVerbGroupAs~: SimilarTo~;
 \item seeAlso~: RelatedTo.
\end{itemize}

Nous avons décidé après coup de conserver ces relations dans le réseau de concepts, et d'en utiliser un sous-ensemble.

\subsubsection{Détail de l'API (\ang{Application's Programmer Interface})}

Nous pouvons faire un certain nombre de requêtes à Conceptnet5. Il existe trois types différents de requêtes~:
\begin{itemize}
 \item Lookup~;
 \item Association~;
 \item Search~;
\end{itemize}
Association permet de calculer la proximité entre deux concepts, ou de récupérer une liste de concepts proches d'un concept donné.

Search permet de récupérer une liste d'arêtes (\ang{edges}) entre concepts, selon les paramètres spécifiés (le plus souvent, on impose le concept de départ \ang{start} ou d'arrivée \ang{end}).

Lookup permet d'analyser un concept (on aura par exemple accès à des listes d'arêtes dans lequel il intervient).


\subsubsection{Utilisation de Conceptnet5}

Nous utilisons en majorité Association et Search.

Étant donné un concept dont nous savons qu'il doit être étendu, nous utilisons conceptnet5 pour créer de nouveaux concepts au sein du réseau, et pour ajouter de nouveaux liens. Une première requête de type Search permet d'avoir accès à un certain nombre de liens vers d'autres concepts, qui sont ajoutés (on ne récupère que les plus pertinents). Une requête de type Association permet de créer de nouveaux liens vers d'autres concepts similaires.

Nous ajoutons alors une arête SimilarTo.

\subsubsection{Freebase}

Freebase\index{Freebase} est une immense base de données sémantiques qui contient beaucoup d'informations, sur des noms propres notamment. Le projet, repris par google mais sous une licence qui laisse les données libres d'accès, de téléchargement et d'utilisation, a lui aussi une API, un peu plus complexe que celle de Conceptnet5. Freebase permet notamment de récupérer des informations sur une personne à partir de son nom, et à peu de frais, de vérifier que cette personne existe bel et bien.



\subsection{Réseau de concepts}

\subsubsection{Structure du réseau de concepts}

\begin{definition}[Réseau de concepts]
Le réseau de concepts\index{Réseau de concepts} est la ``mémoire à long terme'' de notre programme.

Ce réseau est au carrefour entre les réseaux sémantiques et les réseaux de neurones. Les n\oe{}uds du réseau ne représentent pas des concepts à proprement parler~; en revanche, les concepts sont vus comme regroupant des n\oe{}uds fortement interconnectés. Le réseau de concepts est dynamique~; les nœuds évoluent au cours du temps, lors de l'utilisation du programme.
\end{definition}


Le Réseau est constitué de n\oe{}uds. Chaque n\oe{}ud comporte~:
\begin{itemize}
  \item Une étiquette (mot)~;
 \item Une importance conceptuelle (IC) ou profondeur conceptuelle~;
 \item Une activation (A) initialement nulle~;
 \item Un certain nombre de liens à d'autres n\oe{}uds.
\end{itemize}

Chaque lien comporte~:
\begin{itemize}
 \item Une proximité conceptuelle (ou poids W)~;
 \item Une étiquette qui le caractérise.
\end{itemize}


\subsubsection{Construction du réseau}

Étant donné que le réseau de concepts apporte de l'information pour mieux lire le texte, il nous faut la trouver quelque part. Nous avons eu recours à toutes les sources de données précitées. Nous présentons le premier essai grandeur nature de la construction du réseau. Celui-ci, rappelons-le, est censé contenir des données relatives au sport. Chaque concept (joueur de football, mais aussi terme courant comme ``gagner'') y porte une certaine importance conceptuelle, une activation qui est modifiée selon la lecture du texte, et est relié à d'autres concepts par des liens dotés d'un poids (plus celui-ci est élevé, plus le premier concept est susceptible de \textit{faire penser} au deuxième) et d'une relation sur le modèle de Conceptnet5 (RelatedTo, MemberOf, etc).


\paragraph{Récupération de concepts et de noms}

Nous analysons un certain nombre d'articles, ici 3043 issus 7 flux rss différents sur 16 jours.

Nous utilisons en partie les \ang{tokenizers} de \pyt{nltk}, et nous complétons si besoin.

Pour récupérer les noms propres, nous repérons à l'intérieur d'un article les mots marqués comme noms propres par le POS de \pyt{nltk}. Puis nous concaténons deux noms propres successifs. Enfin, nous retirons de la liste des noms propres ceux qui sont contenus à l'intérieur d'autres. Cette étape, largement sous-estimée au début, est cruciale puisqu'elle évite de considérer ``< nom + prénom >'' et ``<prénom>'' comme deux personnes différentes.

Nous comptons aussi la fréquence d'apparition de chaque concept. On obtient 9744 noms et environ 12700 concepts, ce qui est très raisonnable.

\paragraph{Traitement des noms avec Freebase}

Nous n'utilisons pour l'instant l'API Freebase que de fa\c{c}on très superficielle. Chaque nom propre donne lieu à une requête. Si celle-ci aboutit, on analyse le résultat et si celui-ci comporte une donnée (exemple~: à l'envoi de \verb|"scott_jamieson"|, on récupère \verb|"soccer_midfielder"|), on ajoute un nouveau nœud correspondant au réseau, et une arête ``<IsA>'' dont le poids dépend du résultat de la requête.

Dans notre exemple, 676 noms seront rejetés par Freebase~: c'étaient donc des résultats erronés de la première étape.

Environ 7500 nœuds sont créés dans le réseau.

\paragraph{Traitement des concepts avec Conceptnet5}

Pour chaque concept dans la liste, on envoie deux requêtes~: l'une vise à trouver des arêtes qui partent de lui (Search), l'autre à trouver des concepts proches (Association).

En l'absence de résultat, le concept ne sera pas ajouté au réseau.

Dans le cas contraire, de nouvelles arêtes sont créées. Leurs poids dépendent des résultats des requêtes. Pour la requête Association, nous mettons une relation ``<SimilarTo>''.

L'importance conceptuelle d'un nouveau nœud dépend essentiellement de son nombre d'apparitions dans les articles. D'autres possibilités sont envisageables (nombre de synonymes avec WordNet, par exemple).

Dans notre exemple, sur les 12700 concepts, 4800 ne donnent lieu à aucun résultat. Cela peut paraître beaucoup~; en réalité, quand on regarde de plus près la ``<liste noire>'' (que nous stockons), on a par exemple~:``reimagining'', ``money-grabbers'', ``whoscored'', ``impeccable'', ``amateurish'', ``dawkins'', ``rat-a-tat'', ``single-handedly'', ``petered'', ``real-politik'', ``re-tweet'', ``throwing''.

Ces mots sont soit composés (et donc difficiles à analyser), soit résultent d'erreurs du POS-tagger (on voit passer quelques noms propres).

Cette liste pourra bien sûr donner lieu à une analyse plus fine (nouvelles requêtes Freebase).

\`A ce stade, notre réseau possède 24000 nœuds environ, ce qui reste modique.

\paragraph{Extension des nœuds du réseau}

Nous envisageons maintenant, par de nouvelles requêtes, d'étendre les nœuds que nous venons de créer. Le but est que le réseau soit au mieux interconnecté, or de nombreux nœuds résultant des requêtes précédentes ne pointent vers aucun nœud.

D'autres opérations sont possibles.

Nous regardons 5000 de ces nœuds faiblement connectés. Après requêtes, 2300 sont reliés au réseau. (Cette étape a été particulièrement lente).

\paragraph{Suppression des nœuds inutiles}

Malgré les nombreux paramètres servant à filtrer, les requêtes n'ont pas donné des résultats toujours pertinents. Nous choisissons de retirer du réseau les nœuds faiblement reliés (un seul voisin). Ceux-ci seront de toute fa\c{c}on peu utiles lors de la propagation de l'activation.

\`A  ce stade, le réseau comporte 24000 nœuds et 35000 arêtes. Après nettoyage, il reste 17500 nœuds et 29000 arêtes.

Nous disposons maintenant d'un réseau utilisable (en théorie).

\subsubsection{Premiers tests avec ce réseau}

Pour chaque test, nous activons un nœud et laissons l'activation se propager à ses successeurs au cours du temps (sans désactivation). \`A chaque étape (nouvel instant) se produit un calcul d'activation.

\begin{itemize}
 \item \verb|"manchester", 60|
 \begin{verbatim}
  [...]
  Time=10
  comic_book_character : 1 ; furniture : 4
  house : 5 ; manchester : 60
  object : 9 ; place : 11
  thing : 6 ; desk : 4
  location : 21 ; loft : 3
 \end{verbatim}
\item \verb|"francis_lastic", 60|
\begin{verbatim}
  [...]
  Time=5
  coach : 23 ; francis_lastic : 60
\end{verbatim}
\item \verb|"ball_sport", 60|
\begin{verbatim}
  [...]
  Time=10
  ball_sport : 60 ; team_sport : 40
  basketball : 40 ; interest : 7
  football : 40
\end{verbatim}
\end{itemize}

\subsubsection{Choix de programmation et prospectives}

Nous avons d'abord cru que l'utilisation massive de requêtes donnerait des contraintes de temps importantes à nos programmes. C'est en réalité une erreur. La parallélisation des requêtes, que nous avons commencé à effectuer, permet un gain de temps exceptionnel et va nous conduire dans les prochaines semaines à améliorer considérablement le temps d'exécution.

Le réseau est pour le moment enregistré sous forme de JSON Stream, un type de données repris à Conceptnet5. Le nombre de nœuds étant resté raisonnable, nous n'avons pas eu à abandonner prématurément le package \pyt{networkx} qui nous permet de le représenter en mémoire.

Cependant, nous envisageons de passer à une base de données.

Enfin, nous nous orientons maintenant vers des fonctions permettant de construire incrémentalement le réseau, donc d'ajouter les nouveaux concepts et noms propres au fur et à mesure qu'ils sont lus dans les articles (notre but initial était naturellement de construire un réseau ``<de base>'').

\section{Traitement du réseau}

Le réseau de concepts représente, comme nous l'avons déjà dit, la connaissance \textit{a priori} de notre programme sur le domaine étudié. Pour étudier un texte particulier, il s'agit maintenant de faire agir ce texte sur nos connaissances et d'interpréter l'effet que cette lecture a.

Deux méthodes ont été envisagées~: la première étudiait, essentiellement, la différence entre l'état du réseau de concepts avant et après la lecture et devait en déduire les faits importants. La seconde, qui a finalement été retenue, s'appuie sur une activation des concepts dans le réseau, puis une instanciation des concepts fortement activés dans un espace de travail appelé \textit{workspace}\index{Workspace}. Ces concepts instanciés sont par la suite capable d'effectuer des tâches plus complexes relatives à la compréhension du texte.

Quelle que soit la méthode employée, une analyse syntaxique\index{Analyse syntaxique} préalable du texte est nécessaire pour rendre compte de sa compréhension et utiliser au mieux notre réseau de connaissances.

\subsection{Workspace}\index{Workspace}
Le Workspace est une zone où on va stocker des structures formées à partir de concepts et travailler dessus. Par opposition au réseau de concepts, qui représente une connaissance relativement figée du monde, ces structures visent à représenter la compréhension que l'on a pu acquérir du texte.
Par exemple, un type de structure rencontré dans le Workspace serait un graphe décrivant l'évolution de l'état d'un objet au fur et à mesure du temps, d'après ce qu'on a pu extraire du texte.

Au fur et à mesure que l'on découvre les concepts pertinents par rapport au texte grâce au réseau de concepts, on va les ajouter (instancier) au Workspace afin de pouvoir travailler dessus. Ensuite, au moyen de diverses méthodes appelées Workers, on va chercher à les rassembler en des structures porteuses de sens. Les Workers pourront extraire de l'information grâce au texte, sa structure grammaticale, les liens du réseau de connaissances et les structures déjà présentes dans le Workspace.

Par exemple, il y aurait un Worker capable de reconnaître l'identité d'un objet dans le texte. Lorsqu'un concept est instancié dans le Workspace, il cherchera à détecter si ce concept fait référence à un objet récurrent dans le texte, comme par exemple une personne.
En reprenant l'exemple précédent, il y aurait aussi un Worker capable de repérer dans la phrase les descriptions de succession temporelle, et qui créerait les structures dans le Workspace qui reflètent cette évolution. 


\subsection{Analyse syntaxique}\index{Analyse syntaxique}

Cette partie ne constituant pas le c\oe{}ur de notre psc, nous avons décidé d'utiliser un outil déjà codé, faisant partie de la plus grande librairie libre \pyt{nltk}\index{Natural Language ToolKit}.

\subsubsection{Description algorithmique d'une grammaire}\index{Grammaire}
Une grammaire peut être décrite informatiquement de manière très simple. En effet, on peut la voir comme~:
\begin{itemize}
	\item Un ensemble de classes terminales (verbe, nom par exemple) associées à une liste de mots ou éventuellement d'expressions
	\item Un ensemble de règles, souvent sous la forme d'une reconnaissance de motifs, permettant de découper une phrase en plusieurs éléments (et finalement en unités syntaxiques élémentaires).
\end{itemize}

Il y a par conséquent deux manières pour une grammaire d'être incomplète~: soit elle ne connaît pas assez de vocabulaire (manque d'éléments dans les cas terminaux), soit elle ne connaît pas assez de règles. La première faille est assez facile à combler par des requêtes vers WordNet\index{WordNet} (et par ailleurs un outil automatique est inclus dans \pyt{nltk}\index{Natural Language ToolKit})~; il est en revanche assez indispensable d'avoir une grammaire complète en termes de règles car il est beaucoup plus difficile d'inventer des manières de découper une phrase.

\subsubsection{\'Etat du texte en fin d'analyse}
Les grammaires nltk sont classées selon la forme de leur sortie. Elles attendent la liste des mots dans l'ordre dans lequel ils apparaissent dans le texte à étudier, accompagnés d'une liste de classes possibles pour ces mots. En sortie, on peut trouver~:

\begin{itemize}
	\item Un arbre représentant une structure grammaticale possible pour la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase.
	\item La liste de tous les arbres pouvant représenter la structure grammaticale de la phrase, et pour chaque arbre une mesure de la vraisemblance de cette structure.
\end{itemize}

Il y a une grande différence entre les n\oe{}uds non terminaux des arbres en sortie et leurs feuilles. En effet, ces dernières seront les unités syntaxiques présentes à l'origine dans la phrase, tandis que les n\oe{}uds internes représentent la classe grammaticale des bouts de phrases représentés par les sous-arbres dont ils sont la racine.

%Un exemple serait sûrement le bienvenu, je vais voir si je me chauffe pour faire un ParseTree a l'air plausible. (Antonin)
%Cela étant n'hésitez pas à vous chauffer vous aussi.

Les n\oe{}uds internes sont en nombre fini et leurs étiquettes dépendent de si on considère des grammaires sans contexte (capables de séparer des composants autour d'un verbe) ou des grammaires à dépendance (dont le but est d'identifier le sujet et les compléments du verbe central).


\section{Suites envisagées}

Dans l'ensemble le projet avance avec peu de retard sur le plan prévisionnel écrit en novembre dernier. Le réseau de concepts est prêt à être utilisé et notre attention pourra se reporter sur le développement du \textit{workspace}\index{Workspace} et du déploiement de l'analyse syntaxique.\index{Analyse syntaxique}

Cette dernière, que nous avions prise pour à peu près acquise, s'avère requérir plus de travail que prévu. En effet, l'outil que nous avions prévu d'utiliser --- les ``grammaires'' \pyt{nltk}\index{Grammaire} --- soufrent d'un manque de souplesse rédhibitoire. Elles doivent en particulier connaître en avance tous les mots de la phrase pour pouvoir l'analyser, ce qui même en construisant un dictionnaire exhaustif --- qui n'existe pas à l'heure actuelle --- est très limitant pour peu qu'une phrase contienne un néologisme où un nom propre peu connu (voire une simple faute d'orthographe). Nous penchons donc plus actuellement pour des méthodes statistiques plus souples, mais aussi plus difficiles à appréhender.

\vspace{1\baselineskip}

La planification pour le reste du projet est la suivante~:
\begin{description}
	\item[Février~: ]Mise en place de l'analyseur syntaxique. Responsables~: Antonin Angibault, Théophane Hufschmitt, Sarah Fernandes-Pinto-Fachada.
	\item[Février~: ]Fin de la spécification du contenu du \textit{workspace}.
	\item[Février-Avril~: ]Implémentation du \textit{workspace} (donc, essentiellement, de ce qui manque à l'algorithme de résumé pour fonctionner).
	\item[Avril~: ]Génération de résumé et évaluation de ceux-ci.
\end{description}

Les différences essentielles avec la planification initiale proviennent de la complexité inattendue de la grammaire. En revanche, le temps d'exécution prévu pour la production d'un résumé d'article étant long, nous produirons finalement peu de résumés à évaluer, cela nous permet donc de les évaluer à la main et de gagner du temps sur cette partie de la planification initiale.

Globalement nous pensons être capables de mener le projet à bien comme prévu. Les sections suivantes décrivent les points les plus prêts à être mis en place.

\subsection{Apprentissage du réseau de concepts}
Le réseau de concept représente un \textit{a priori} sur le monde de notre programme. Il est donc normal que cette structure évolue au fur et à mesure des textes lus.

Les fonctions de construction du réseau peuvent sans mal être appelées après la génération du résumé pour y apporter des modifications pérennes.

\subsection{\'Evaluation des résumés}
Le faible nombre de résumés que nous prévoyons de produire nous permettra d'utiliser la méthode la plus simple parmi celles qui étaient mentionnées dans la proposition détaillée~: la comparaison à un résumé déclaré bon car produit par un humain.

L'idée est la suivante~: l'un de nous résumera un texte, nous emploierons notre programme pour résumer le même texte, et enfin nous utiliserons un simple algorithme statistique (basé sur TF-IDF) pour résumer ce même texte. Nous comparerons ensuite ces résumés du point de vue de l'information qu'ils contiennent~: sont-ils des images assez fidèles du document original~?

Compte-tenu de la simplicité de l'algorithme statistique, nous espérons que le résumé produit par notre programme se situera quelque part entre le résumé statistique et le résumé humain.

\section{Références bibliographiques}

\nocite{*}
\printbibliography{}

\appendix

\printindex


\end{document}

##############################################
##CODE FROM CONCEPTNET5 slightly modified
################################################



# -*- coding: utf-8 -*-
from __future__ import print_function, unicode_literals

"""
Tools for working with English text and reducing it to a normal form.

In English, we remove a very small number of stopwords, and then apply a
modified version of Morphy, the stemmer (lemmatizer) used in WordNet.  The
modifications mostly involve heuristics for when to apply noun or verb
transformations to words whose part of speech is ambiguous.
"""
from nltk.corpus import wordnet
import re
from abstracter.parsers.tokenizer import tokenize, normalizer_tokenize,get_named_entities,concepts_tokenize
morphy = wordnet._morphy


STOPWORDS = ['the', 'a', 'an']

EXCEPTIONS = {
    # Avoid obsolete and obscure roots, the way lexicographers don't.
    'wrought': 'wrought',   # not 'work'
    'media': 'media',       # not 'medium'
    'installed': 'install', # not 'instal'
    'installing': 'install',# not 'instal'
    'synapses': 'synapse',  # not 'synapsis'
    'soles': 'sole',        # not 'sol'
    'pubes': 'pube',        # not 'pubis'
    'dui': 'dui',           # not 'duo'
    'taxis': 'taxi',        # not 'taxis'

    # Work around errors that Morphy makes.
    'alas': 'alas',
    'corps': 'corps',
    'cos': 'cos',
    'enured': 'enure',
    'fiver': 'fiver',
    'hinder': 'hinder',
    'lobed': 'lobe',
    'offerer': 'offerer',
    'outer': 'outer',
    'sang': 'sing',
    'singing': 'sing',
    'solderer': 'solderer',
    'tined': 'tine',
    'twiner': 'twiner',
    'us': 'us',

    # Stem common nouns whose plurals are apparently ambiguous
    'teeth': 'tooth',
    'things': 'thing',
    'people': 'person',

    # Tokenization artifacts
    'wo': 'will',
    'ca': 'can',
    "n't": 'not',
}

AMBIGUOUS_EXCEPTIONS = {
    # Avoid nouns that shadow more common verbs.
    'am': 'be',
    'as': 'as',
    'are': 'be',
    'ate': 'eat',
    'bent': 'bend',
    'drove': 'drive',
    'fell': 'fall',
    'felt': 'feel',
    'found': 'find',
    'has': 'have',
    'lit': 'light',
    'lost': 'lose',
    'sat': 'sit',
    'saw': 'see',
    'sent': 'send',
    'shook': 'shake',
    'shot': 'shoot',
    'slain': 'slay',
    'spoke': 'speak',
    'stole': 'steal',
    'sung': 'sing',
    'thought': 'think',
    'tore': 'tear',
    'was': 'be',
    'won': 'win',
    'feed': 'feed',
}




def _word_badness(word):
    """
    Assign a heuristic to possible outputs from Morphy. Minimizing this
    heuristic avoids incorrect stems.
    """
    if word.endswith('e'):
        return len(word) - 2
    elif word.endswith('ess'):
        return len(word) - 10
    elif word.endswith('ss'):
        return len(word) - 4
    else:
        return len(word)


def _morphy_best(word, pos=None):
    """
    Get the most likely stem for a word using Morphy, once the input has been
    pre-processed by morphy_stem().
    """
    results = []
    if pos is None:
        pos = 'nvar'
    for pos_item in pos:
        results.extend(morphy(word, pos_item))
    if not results:
        return None
    results.sort(key=lambda x: _word_badness(x))
    return results[0]


def morphy_stem(word, pos=None):
    """
    Get the most likely stem for a word. If a part of speech is supplied,
    the stem will be more accurate.

    Valid parts of speech are:

    - 'n' or 'NN' for nouns
    - 'v' or 'VB' for verbs
    - 'a' or 'JJ' for adjectives
    - 'r' or 'RB' for adverbs

    Any other part of speech will be treated as unknown.
    """
    # FIXME: strip punctuation that may still be attached to the word
    word = word.lower()
    if pos is not None:
        if pos.startswith('NN'):
            pos = 'n'
        elif pos.startswith('VB'):
            pos = 'v'
        elif pos.startswith('JJ'):
            pos = 'a'
        elif pos.startswith('RB'):
            pos = 'r'
    if pos is None and word.endswith('ing') or word.endswith('ed'):
        pos = 'v'
    if pos is not None and pos not in 'nvar':
        pos = None
    if word in EXCEPTIONS:
        return EXCEPTIONS[word]
    if pos is None:
        if word in AMBIGUOUS_EXCEPTIONS:
            return AMBIGUOUS_EXCEPTIONS[word]
    return _morphy_best(word, pos) or word


def good_lemma(lemma):
    """
    Filter for lemmas that are not stopwords, and are not punctuation.
    """
    return lemma and lemma not in STOPWORDS and lemma[0].isalnum()


def normalize(text,named_entities):
    """
    Get a list of word stems that appear in the text. Stopwords and an initial
    'to' will be stripped, unless this leaves nothing in the stem.

    >>> normalize_as_list('the dog')
    ['dog']
    >>> normalize_as_list('big dogs')
    ['big', 'dog']
    >>> normalize_as_list('the')
    ['the']
    """
    pieces = [[morphy_stem(word,pos),pos] for word,pos in normalizer_tokenize(text,named_entities)]
    pieces = [[piece,pos] for piece,pos in pieces if good_lemma(piece)]
    if not pieces:
        return [text]
    if pieces[0] == 'to':
        pieces = pieces[1:]
    return pieces


#def normalize(text):
#    """
#    Get a string made from the non-stopword word stems in the text. See
#    normalize_as_list().
#    """
#    return untokenize(normalize_as_list(text))


#def normalize_topic(topic):
#    """
#    Get a canonical representation of a Wikipedia topic, which may include
#    a disambiguation string in parentheses.
#    Returns (name, disambig), where "name" is the normalized topic name,
#   and "disambig" is a string corresponding to the disambiguation text or
#   None.
#    """
#    # find titles of the form Foo (bar)
#    topic = topic.replace('_', ' ')
#    match = re.match(r'([^(]+) \(([^)]+)\)', topic)
#    if not match:
#        return normalize(topic), None
#    else:
#        return normalize(match.group(1)), 'n/' + match.group(2).strip(' _')



def _links_to(entity_list,name):
    """
    :param entity_list: List of entities (string)
    :type name: str
    :rtype: boolean
    """
    if entity_list:
        for e in entity_list:
            if name in e:
                return True
    return False


def retrieve_concepts(text):
    names=list(s.lower() for s in get_named_entities(text))
    words=concepts_tokenize(text)#words with POS, without names
    namescopy=names.copy()
    concepts=[]
    #suppress duplicated names and add names to concept
    for name in namescopy:
        names.remove(name)
        if not _links_to(names,name):
            concepts.append(name)
            names.append(name)
    #here : concepts contains all useful names
    #morph other words
    pieces = [morphy_stem(word,pos) for word,pos in words]
    pieces = [piece for piece in pieces if good_lemma(piece)]
    if not pieces:
        return []
    for word in pieces:
        if word not in concepts:
            concepts.append(word)
    #sort the list
    concepts.sort()
    return concepts


if __name__=="__main__":
    pass
